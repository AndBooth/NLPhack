{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning models for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    '''Takes a word embedding and transforms documents into embedded vectors\n",
    "        Can specify to use a weighted sum by fitting a Tfidf Vectorizer and using those weights'''\n",
    "    \n",
    "    def __init__(self, wv, weighted_vec=True, max_df=1., min_df=int(1)):\n",
    "        self.wv = wv\n",
    "        self.weighted_vec = weighted_vec\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.dim = wv.vector_size\n",
    "        \n",
    "    def fit(self, X):\n",
    "        if self.weighted_vec:\n",
    "            self.tfidf_vec = TfidfVectorizer(max_df=self.max_df, min_df=self.min_df)\n",
    "            self.tfidf_vec.fit(X)\n",
    "            # if a word was never seen - it must be at least as infrequent\n",
    "            # as any of the known words - so the default idf is the max of \n",
    "            # known idf's\n",
    "            \n",
    "            max_idf = max(self.tfidf_vec.idf_) \n",
    "            self.tfidf_dict = defaultdict(lambda: max_idf, zip(self.tfidf_vec.get_feature_names(), self.tfidf_vec.idf_))\n",
    "        return self\n",
    "        \n",
    "    def DocToWordVector(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        vec = np.zeros(self.dim).reshape((1, self.dim))\n",
    "        count = 0.\n",
    "    \n",
    "        for word in tokens:\n",
    "            if self.weighted_vec:\n",
    "                weight = self.tfidf_dict[word]\n",
    "            else:\n",
    "                weight = 1\n",
    "            try:\n",
    "                vec += self.wv[word].reshape((1, self.dim)) * weight\n",
    "                count += 1.\n",
    "            except KeyError:      # handling the case where the token is not\n",
    "                                  # in the word embedding\n",
    "                continue\n",
    "    \n",
    "        if count != 0:\n",
    "            vec = vec / count\n",
    "        return vec\n",
    "            \n",
    "    def transform(self, X):\n",
    "        transformed_X = np.vstack(np.array(list(map(self.DocToWordVector, X))))\n",
    "        return transformed_X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = metrics.f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/train.csv')\n",
    "test = pd.read_csv('../../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedding file using google news word2vec\n",
    "news_path = '../../data/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Following <a href='https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings'>Dieter</a> on a better approach of preprocessing data when one has a word embedding. Essentially only perform steps to increase the vocab covered by the word embedding and do not mindlessly apply general text preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:06<00:00, 211445.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:06<00:00, 194898.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': 261930, 'did': 33489, 'Quebec': 97, 'nationalists': 91, 'see': 9003}\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from training data\n",
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    covered_word_count = 0\n",
    "    oov_word_count = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            covered_word_count += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            oov_word_count += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(covered_word_count / (covered_word_count + oov_word_count)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 508823/508823 [00:01<00:00, 263722.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 24.31% of vocab\n",
      "Found embeddings for  78.75% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 403183),\n",
       " ('a', 402682),\n",
       " ('of', 330825),\n",
       " ('and', 251973),\n",
       " ('India?', 16384),\n",
       " ('it?', 12900),\n",
       " ('do?', 8753),\n",
       " ('life?', 7753),\n",
       " ('you?', 6295),\n",
       " ('me?', 6202)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]\n",
    "# Lots of stop words and punctuation needs to be dealt with also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('?' in embeddings_index)\n",
    "print('&' in embeddings_index)\n",
    "# Remove all punctuation except &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(x):\n",
    "    '''Removes all punctuation apart from &'''\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:20<00:00, 62806.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:06<00:00, 203476.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: remove_punct(x))\n",
    "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 253623/253623 [00:01<00:00, 235429.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 57.38% of vocab\n",
      "Found embeddings for  89.99% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 406298),\n",
       " ('a', 403852),\n",
       " ('of', 332964),\n",
       " ('and', 254081),\n",
       " ('2017', 8781),\n",
       " ('2018', 7373),\n",
       " ('10', 6642),\n",
       " ('12', 3694),\n",
       " ('20', 2942),\n",
       " ('100', 2883)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]\n",
    "# Numbers need to be dealt with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out numbers are in word2vec but represented with hashes i.e. 45 becomes ## and 345 becomes ### etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:23<00:00, 54785.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:06<00:00, 196706.28it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 242997/242997 [00:01<00:00, 235712.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 60.41% of vocab\n",
      "Found embeddings for  90.75% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 406298),\n",
       " ('a', 403852),\n",
       " ('of', 332964),\n",
       " ('and', 254081),\n",
       " ('favourite', 1247),\n",
       " ('bitcoin', 987),\n",
       " ('colour', 976),\n",
       " ('doesnt', 918),\n",
       " ('centre', 886),\n",
       " ('Quorans', 858),\n",
       " ('cryptocurrency', 822),\n",
       " ('Snapchat', 807),\n",
       " ('travelling', 705),\n",
       " ('counselling', 634),\n",
       " ('btech', 632),\n",
       " ('didnt', 600),\n",
       " ('Brexit', 493),\n",
       " ('cryptocurrencies', 481),\n",
       " ('blockchain', 474),\n",
       " ('behaviour', 468),\n",
       " ('upvotes', 432),\n",
       " ('isnt', 431),\n",
       " ('programme', 402),\n",
       " ('Redmi', 379),\n",
       " ('realise', 371),\n",
       " ('defence', 364),\n",
       " ('KVPY', 349),\n",
       " ('Paytm', 334),\n",
       " ('organisation', 316),\n",
       " ('grey', 299)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:30]\n",
    "# Time to remove stop words\n",
    "# and UK/US language differences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'behaviour': 'behavior',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium',\n",
    "                'Snapchat': 'social medium',\n",
    "                'Pinterest': 'social medium',\n",
    "                'aluminium': 'aluminum',\n",
    "                'bitcoin': 'cryptography currency',\n",
    "                'cryptocurrency': 'cryptography currency',\n",
    "                'cryptocurrencies': 'cryptography currency',\n",
    "                'blockchain': 'cryptography currency',\n",
    "                'Blockchain': 'cryptography currency',\n",
    "                'Ethereum': 'cryptography currency',\n",
    "                'ethereum': 'cryptography currency',\n",
    "                'realise': 'realize',\n",
    "                'defence': 'defense'\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:19<00:00, 68311.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:06<00:00, 214743.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:05<00:00, 243932.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:05<00:00, 228824.02it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 242897/242897 [00:01<00:00, 200672.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 60.44% of vocab\n",
      "Found embeddings for  99.00% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quorans', 858),\n",
       " ('btech', 632),\n",
       " ('Brexit', 493),\n",
       " ('upvotes', 432),\n",
       " ('programme', 402),\n",
       " ('Redmi', 379),\n",
       " ('KVPY', 349),\n",
       " ('Paytm', 334),\n",
       " ('grey', 299),\n",
       " ('currencys', 282),\n",
       " ('mtech', 281),\n",
       " ('Btech', 262),\n",
       " ('honours', 252),\n",
       " ('learnt', 248),\n",
       " ('upvote', 247),\n",
       " ('licence', 242),\n",
       " ('…', 210),\n",
       " ('Whatis', 209),\n",
       " ('bcom', 199),\n",
       " ('Isnt', 192),\n",
       " ('favour', 175),\n",
       " ('INTJ', 173),\n",
       " ('cheque', 159),\n",
       " ('INFJ', 157),\n",
       " ('aadhar', 150),\n",
       " ('judgement', 145),\n",
       " ('Fiverr', 143),\n",
       " ('modelling', 143),\n",
       " ('Xiaomi', 140),\n",
       " ('Coursera', 137),\n",
       " ('Fortnite', 130),\n",
       " ('OnePlus', 125),\n",
       " ('recognise', 124),\n",
       " ('Lyft', 124),\n",
       " ('wasnt', 123),\n",
       " ('UCEED', 123),\n",
       " ('AFCAT', 122),\n",
       " ('jewellery', 121),\n",
       " ('hasnt', 117),\n",
       " ('practise', 113),\n",
       " ('WeChat', 112),\n",
       " ('INFP', 111),\n",
       " ('travelled', 111),\n",
       " ('vape', 111),\n",
       " ('analyse', 110),\n",
       " ('GDPR', 107),\n",
       " ('demonetisation', 106),\n",
       " ('Nodejs', 105),\n",
       " ('UPSEE', 105),\n",
       " ('recognised', 105),\n",
       " ('Coinbase', 104),\n",
       " ('programmes', 104),\n",
       " ('upvoted', 102),\n",
       " ('BNBR', 99),\n",
       " ('Manaphy', 99),\n",
       " ('Machedo', 99),\n",
       " ('neighbour', 98),\n",
       " ('litre', 98),\n",
       " ('sulphuric', 97),\n",
       " ('valency', 95),\n",
       " ('neighbours', 94),\n",
       " ('–', 93),\n",
       " ('Boruto', 93),\n",
       " ('tyres', 92),\n",
       " ('Udacity', 91),\n",
       " ('selfies', 90),\n",
       " ('tyre', 89),\n",
       " ('DCEU', 89),\n",
       " ('specialisation', 89),\n",
       " ('traveller', 84),\n",
       " ('organised', 84),\n",
       " ('downvote', 83),\n",
       " ('selfie', 83),\n",
       " ('##°', 83),\n",
       " ('offence', 82),\n",
       " ('Laravel', 82),\n",
       " ('SJWs', 81),\n",
       " ('vapour', 79),\n",
       " ('Qoura', 78),\n",
       " ('litres', 77),\n",
       " ('fibre', 77),\n",
       " ('sinx', 77),\n",
       " ('ICOs', 76),\n",
       " ('aeroplane', 76),\n",
       " ('hairfall', 73),\n",
       " ('civilisation', 72),\n",
       " ('CTMU', 72),\n",
       " ('utilise', 71),\n",
       " ('INTPs', 70),\n",
       " ('humour', 70),\n",
       " ('archaeology', 68),\n",
       " ('JIIT', 68),\n",
       " ('sulphur', 67),\n",
       " ('LNMIIT', 67),\n",
       " ('masterbate', 66),\n",
       " ('Upwork', 66),\n",
       " ('y2', 65),\n",
       " ('organise', 65),\n",
       " ('Zerodha', 65),\n",
       " ('neurotypicals', 65)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('defense' in embeddings_index)\n",
    "print('programs' in embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907792091369629),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.661863386631012),\n",
       " ('passion', 0.6100709438323975),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('Ilove', 0.5702950954437256),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547305345535278),\n",
       " ('absolutely_adore', 0.5536840558052063)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.similar_by_word('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.question_text.values\n",
    "y = train.target.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embed_transformer = EmbeddingVectorizer(embeddings_index, weighted_vec=False)\n",
    "X_train_wv = embed_transformer.fit_transform(X_train)\n",
    "X_test_wv = embed_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tprjo\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_wv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121110   1422]\n",
      " [  5725   2356]]\n",
      "0.3973353571127414\n"
     ]
    }
   ],
   "source": [
    "test_predictions = clf.predict(X_test_wv)\n",
    "print(metrics.confusion_matrix(y_test, test_predictions))\n",
    "print(metrics.f1_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.21, 'f1': 0.539636285143813}\n"
     ]
    }
   ],
   "source": [
    "predictions_proba = clf.predict_proba(X_test_wv)\n",
    "search_result = threshold_search(y_test, predictions_proba[:,1])\n",
    "print(search_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130613,)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_proba[:,1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP on question vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "X_train_scaled = scl.fit_transform(X_train_wv)\n",
    "X_test_scaled = scl.transform(X_test_wv)\n",
    "\n",
    "INPUT_DIM = X_train_wv.shape[1]\n",
    "LAYER_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(LAYER_SIZE, input_dim=INPUT_DIM, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(LAYER_SIZE, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: early stopping\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')\n",
    "model.fit(X_train_scaled, y=y_train, batch_size=1024, \n",
    "          epochs=20, verbose=0, \n",
    "          validation_data=(X_test_scaled, y_test), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6299591891349434"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_predict = model.predict_classes(X_test_scaled)\n",
    "metrics.f1_score(y_test, keras_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.33, 'f1': 0.6567847536198705}\n"
     ]
    }
   ],
   "source": [
    "keras_probas = model.predict(X_test_scaled)\n",
    "keras_search = threshold_search(y_test, keras_probas)\n",
    "print(keras_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
