{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning models for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    '''Takes a word embedding and transforms documents into embedded vectors\n",
    "        Can specify to use a weighted sum by fitting a Tfidf Vectorizer and using those weights'''\n",
    "    \n",
    "    def __init__(self, wv, weighted_vec=True, max_df=1., min_df=int(1)):\n",
    "        self.wv = wv\n",
    "        self.weighted_vec = weighted_vec\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.dim = wv.vector_size\n",
    "        \n",
    "    def fit(self, X):\n",
    "        if self.weighted_vec:\n",
    "            self.tfidf_vec = TfidfVectorizer(max_df=self.max_df, min_df=self.min_df)\n",
    "            self.tfidf_vec.fit(X)\n",
    "            # if a word was never seen - it must be at least as infrequent\n",
    "            # as any of the known words - so the default idf is the max of \n",
    "            # known idf's\n",
    "            \n",
    "            max_idf = max(self.tfidf_vec.idf_) \n",
    "            self.tfidf_dict = defaultdict(lambda: max_idf, zip(self.tfidf_vec.get_feature_names(), self.tfidf_vec.idf_))\n",
    "        return self\n",
    "        \n",
    "    def DocToWordVector(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        vec = np.zeros(self.dim).reshape((1, self.dim))\n",
    "        count = 0.\n",
    "    \n",
    "        for word in tokens:\n",
    "            if self.weighted_vec:\n",
    "                weight = self.tfidf_dict[word]\n",
    "            else:\n",
    "                weight = 1\n",
    "            try:\n",
    "                vec += self.wv[word].reshape((1, self.dim)) * weight\n",
    "                count += 1.\n",
    "            except KeyError:      # handling the case where the token is not\n",
    "                                  # in the word embedding\n",
    "                continue\n",
    "    \n",
    "        if count != 0:\n",
    "            vec = vec / count\n",
    "        return vec\n",
    "            \n",
    "    def transform(self, X):\n",
    "        transformed_X = np.vstack(np.array(list(map(self.DocToWordVector, X))))\n",
    "        return transformed_X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = metrics.f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/train.csv')\n",
    "test = pd.read_csv('../../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedding file using google news word2vec\n",
    "news_path = '../../data/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Following <a href='https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings'>Dieter</a> on a better approach of preprocessing data when one has a word embedding. Essentially only perform steps to increase the vocab covered by the word embedding and do not mindlessly apply general text preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 351481.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 346088.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': 261930, 'did': 33489, 'Quebec': 97, 'nationalists': 91, 'see': 9003}\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from training data\n",
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    covered_word_count = 0\n",
    "    oov_word_count = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            covered_word_count += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            oov_word_count += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(covered_word_count / (covered_word_count + oov_word_count)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 508823/508823 [00:01<00:00, 455507.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 24.31% of vocab\n",
      "Found embeddings for  78.75% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 403183),\n",
       " ('a', 402682),\n",
       " ('of', 330825),\n",
       " ('and', 251973),\n",
       " ('India?', 16384),\n",
       " ('it?', 12900),\n",
       " ('do?', 8753),\n",
       " ('life?', 7753),\n",
       " ('you?', 6295),\n",
       " ('me?', 6202)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]\n",
    "# Lots of stop words and punctuation needs to be dealt with also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('?' in embeddings_index)\n",
    "print('&' in embeddings_index)\n",
    "# Remove all punctuation except &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(x):\n",
    "    '''Removes all punctuation apart from &'''\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:09<00:00, 131869.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 375112.11it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: remove_punct(x))\n",
    "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 253623/253623 [00:00<00:00, 420308.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 57.38% of vocab\n",
      "Found embeddings for  89.99% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 406298),\n",
       " ('a', 403852),\n",
       " ('of', 332964),\n",
       " ('and', 254081),\n",
       " ('2017', 8781),\n",
       " ('2018', 7373),\n",
       " ('10', 6642),\n",
       " ('12', 3694),\n",
       " ('20', 2942),\n",
       " ('100', 2883)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]\n",
    "# Numbers need to be dealt with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out numbers are in word2vec but represented with hashes i.e. 45 becomes ## and 345 becomes ### etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:11<00:00, 109143.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 344254.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 242997/242997 [00:00<00:00, 357242.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 60.41% of vocab\n",
      "Found embeddings for  90.75% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 406298),\n",
       " ('a', 403852),\n",
       " ('of', 332964),\n",
       " ('and', 254081),\n",
       " ('favourite', 1247),\n",
       " ('bitcoin', 987),\n",
       " ('colour', 976),\n",
       " ('doesnt', 918),\n",
       " ('centre', 886),\n",
       " ('Quorans', 858),\n",
       " ('cryptocurrency', 822),\n",
       " ('Snapchat', 807),\n",
       " ('travelling', 705),\n",
       " ('counselling', 634),\n",
       " ('btech', 632),\n",
       " ('didnt', 600),\n",
       " ('Brexit', 493),\n",
       " ('cryptocurrencies', 481),\n",
       " ('blockchain', 474),\n",
       " ('behaviour', 468)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]\n",
    "# Time to remove stop words\n",
    "# and UK/US language differences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'behaviour': 'behavior',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium',\n",
    "                'Snapchat': 'social medium',\n",
    "                'Pinterest': 'social medium',\n",
    "                'aluminium': 'aluminum',\n",
    "                'bitcoin': 'cryptography currency',\n",
    "                'cryptocurrency': 'cryptography currency',\n",
    "                'cryptocurrencies': 'cryptography currency',\n",
    "                'blockchain': 'cryptography currency',\n",
    "                'Blockchain': 'cryptography currency',\n",
    "                'Ethereum': 'cryptography currency',\n",
    "                'ethereum': 'cryptography currency',\n",
    "                'realise': 'realize',\n",
    "                'defence': 'defense'\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:09<00:00, 135898.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:02<00:00, 502695.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 335866.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 396367.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 242897/242897 [00:00<00:00, 430593.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 60.44% of vocab\n",
      "Found embeddings for  99.00% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quorans', 858),\n",
       " ('btech', 632),\n",
       " ('Brexit', 493),\n",
       " ('upvotes', 432),\n",
       " ('programme', 402),\n",
       " ('Redmi', 379),\n",
       " ('KVPY', 349),\n",
       " ('Paytm', 334),\n",
       " ('grey', 299),\n",
       " ('currencys', 282),\n",
       " ('mtech', 281),\n",
       " ('Btech', 262),\n",
       " ('honours', 252),\n",
       " ('learnt', 248),\n",
       " ('upvote', 247),\n",
       " ('licence', 242),\n",
       " ('…', 210),\n",
       " ('Whatis', 209),\n",
       " ('bcom', 199),\n",
       " ('Isnt', 192),\n",
       " ('favour', 175),\n",
       " ('INTJ', 173),\n",
       " ('cheque', 159),\n",
       " ('INFJ', 157),\n",
       " ('aadhar', 150),\n",
       " ('judgement', 145),\n",
       " ('Fiverr', 143),\n",
       " ('modelling', 143),\n",
       " ('Xiaomi', 140),\n",
       " ('Coursera', 137),\n",
       " ('Fortnite', 130),\n",
       " ('OnePlus', 125),\n",
       " ('recognise', 124),\n",
       " ('Lyft', 124),\n",
       " ('wasnt', 123),\n",
       " ('UCEED', 123),\n",
       " ('AFCAT', 122),\n",
       " ('jewellery', 121),\n",
       " ('hasnt', 117),\n",
       " ('practise', 113),\n",
       " ('WeChat', 112),\n",
       " ('INFP', 111),\n",
       " ('travelled', 111),\n",
       " ('vape', 111),\n",
       " ('analyse', 110),\n",
       " ('GDPR', 107),\n",
       " ('demonetisation', 106),\n",
       " ('Nodejs', 105),\n",
       " ('UPSEE', 105),\n",
       " ('recognised', 105),\n",
       " ('Coinbase', 104),\n",
       " ('programmes', 104),\n",
       " ('upvoted', 102),\n",
       " ('BNBR', 99),\n",
       " ('Manaphy', 99),\n",
       " ('Machedo', 99),\n",
       " ('neighbour', 98),\n",
       " ('litre', 98),\n",
       " ('sulphuric', 97),\n",
       " ('valency', 95),\n",
       " ('neighbours', 94),\n",
       " ('–', 93),\n",
       " ('Boruto', 93),\n",
       " ('tyres', 92),\n",
       " ('Udacity', 91),\n",
       " ('selfies', 90),\n",
       " ('tyre', 89),\n",
       " ('DCEU', 89),\n",
       " ('specialisation', 89),\n",
       " ('traveller', 84),\n",
       " ('organised', 84),\n",
       " ('downvote', 83),\n",
       " ('selfie', 83),\n",
       " ('##°', 83),\n",
       " ('offence', 82),\n",
       " ('Laravel', 82),\n",
       " ('SJWs', 81),\n",
       " ('vapour', 79),\n",
       " ('Qoura', 78),\n",
       " ('litres', 77),\n",
       " ('fibre', 77),\n",
       " ('sinx', 77),\n",
       " ('ICOs', 76),\n",
       " ('aeroplane', 76),\n",
       " ('hairfall', 73),\n",
       " ('civilisation', 72),\n",
       " ('CTMU', 72),\n",
       " ('utilise', 71),\n",
       " ('INTPs', 70),\n",
       " ('humour', 70),\n",
       " ('archaeology', 68),\n",
       " ('JIIT', 68),\n",
       " ('sulphur', 67),\n",
       " ('LNMIIT', 67),\n",
       " ('masterbate', 66),\n",
       " ('Upwork', 66),\n",
       " ('y2', 65),\n",
       " ('organise', 65),\n",
       " ('Zerodha', 65),\n",
       " ('neurotypicals', 65)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.question_text.values\n",
    "y = train.target.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embed_transformer = EmbeddingVectorizer(embeddings_index, weighted_vec=False)\n",
    "X_train_wv = embed_transformer.fit_transform(X_train)\n",
    "X_test_wv = embed_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tprjo\\AppData\\Local\\conda\\conda\\envs\\nlphack\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_wv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121084   1448]\n",
      " [  5651   2430]]\n",
      "0.4063884940212393\n"
     ]
    }
   ],
   "source": [
    "test_predictions = clf.predict(X_test_wv)\n",
    "print(metrics.confusion_matrix(y_test, test_predictions))\n",
    "print(metrics.f1_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.23, 'f1': 0.5436260464071015}\n"
     ]
    }
   ],
   "source": [
    "predictions_proba = clf.predict_proba(X_test_wv)\n",
    "search_result = threshold_search(y_test, predictions_proba[:,1])\n",
    "print(search_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs better after this preprocessing than the general preprocessing in notebook 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP on question vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "X_train_scaled = scl.fit_transform(X_train_wv)\n",
    "X_test_scaled = scl.transform(X_test_wv)\n",
    "\n",
    "INPUT_DIM = X_train_wv.shape[1]\n",
    "LAYER_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "K.clear_session()\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(LAYER_SIZE, input_dim=INPUT_DIM, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(LAYER_SIZE, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38fdea4106a4205b24e6354f448f460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=20, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203afbfdb2d440f58dd44e2c8ffd5272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec0e242ee7c4f0fba94bb82aabb9ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223d04c149b642e1841260bd75a92629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493bc6fbbc2d419283661e19402b9a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e501a64b8ae418aacedeb5a3c5c70f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09cf1d33e9a418bb2a2de29a6490e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1f848a9b114de5b15d070cba7af563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766d1ad284ea4e9aa8a292cee9b80167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0067fb873a84338a1c17fa801f93156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582b693afc354f2394a30bb777d9cbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=1175509, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13108dbc16e24342baef5669128b9e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=1175509, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07403b60150e43fca4343fb986c36b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=1175509, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: early stopping\n",
      "\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')\n",
    "model.fit(X_train_scaled, y=y_train, batch_size=1024, \n",
    "          epochs=20, verbose=0, \n",
    "          validation_data=(X_test_scaled, y_test), callbacks=[earlystop, TQDMNotebookCallback(leave_inner=True, leave_outer=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6154272645366968"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_predict = model.predict_classes(X_test_scaled)\n",
    "metrics.f1_score(y_test, keras_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.28, 'f1': 0.6525347689546882}\n"
     ]
    }
   ],
   "source": [
    "keras_probas = model.predict(X_test_scaled)\n",
    "keras_search = threshold_search(y_test, keras_probas)\n",
    "print(keras_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot better than anything before, deep learning is the way forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
